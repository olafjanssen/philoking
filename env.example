# LLM Provider Configuration
PROVIDER=ollama  # "ollama" or "openai"
MODEL=llama2    # Model name (e.g., llama2, codellama, mistral)

# Ollama Configuration
OLLAMA_URL=http://localhost:11434

# OpenAI Configuration (if using OpenAI)
LLM_API_KEY=your_openai_api_key_here

# Kafka Configuration (optional, defaults to localhost:9092)
KAFKA_BROKERS=localhost:9092

# Web Server Configuration (optional)
WEB_HOST=localhost
WEB_PORT=8080

